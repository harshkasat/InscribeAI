<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reranking-RAG System Using Llama-Index, Llama 3 and Qdrant</title>
  </head>

  <body>
      <h1>Build an Advanced Reranking-RAG System Using Llama-Index, Llama 3 and Qdrant</h1>
      <h2>Introduction</h2>
      <p>
        LLMs, despite their ability to generate meaningful and grammatically correct text, face a challenge known as hallucination.
        Hallucination in LLMs refers to their tendency to confidently produce incorrect answers, creating false information that
        can appear convincing. To address hallucination, Fact Checking is crucial. One approach to prototyping LLMs for
        Fact Checking involves three methods:
      </p>
      <ol>
        <li>Identify the type of hallucination</li>
        <li>Rank answers based on factual evidence</li>
        <li>Provide user interface to flag/correct hallucinations</li>
      </ol>
      <p>In this context, we will utilize RAG (Retrieval Augmented Generation) to mitigate hallucination</p>
      <h2>What Is RAG ?</h2>
      <p>
        RAG = DenseVector Retrieval (R) + Incontext Learning (AG)<br>
        Retrieval: Find References for the question asked in your Document.<br>
        Augmented: Add References to your Prompt.<br>
        Generation: Improve answers to the question asked.
      </p>
      <h2>Why RAG Before Fine-Tuning (Order of Operation)?</h2>
      <p>
        The optimization workflow gives a summary of the approach that can be used based on the following two factors:
      <ol>
        <li>Fine-Tuning with task specific data can be slow and expensive</li>
        <li>Fine-Tuning can be more error prone</li>
      </ol>
      </p>
      <h2>RAG Data Stack</h2>
      <ol>
        <li>Load Language Data</li>
        <li>Process Language Data</li>
        <li>Embed Language Data</li>
        <li>Load Vectors into Database</li>
      </ol>
      <h2>Stages Involved in RAG</h2>
      <ol>
        <li>Chunking</li>
        <li>Embedding</li>
        <li>Similarity Search</li>
        <li>Retrieval</li>
        <li>Augmentation</li>
        <li>Generation</li>
      </ol>
      <h2>Our RAG stack is built using Llama-Index, Qdrant, and Llama 3.</h2>
      <h3>What Is Llama-Index ?</h3>
      <p>
        Llama-Index serves as a framework designed for developing LLM applications enriched with context. Context augmentation
        involves utilizing LLMs with your private or domain-specific data. Some popular applications of this framework
        include: <br>
        - Question Answering <br>
        - Code Completion <br>
        - Summarization
      </p>
      <p>
        Llama-Index offers a comprehensive set of tools to facilitate the development of these applications, from initial prototypes
        to production-ready solutions. These tools enable data ingestion and processing, as well as the implementation of
        sophisticated query workflows that combine data access with LLM-based prompting.
      </p>
      <h3>Major Enhancements</h3>
      <ul>
        <li>ServiceContext is deprecated: Every LlamaIndex user is familiar with ServiceContext, which has gradually
          become outdated and cumbersome for managing LLMs, embeddings, chunk sizes, callbacks, and other functionalities.
          Consequently, we are fully deprecating it; you can now either specify arguments directly or set a default.</li>
        <li>Revamped Folder Structure:</li><br>
        LlamaHub will serve as the central hub for all integrations.
      </ul>
      <h3>Llama 3</h3>
      <p>
        Metaï¿½s Llama 3 is the latest version of the open-access Llama series, accessible through Hugging Face. It serves as the
        language model for response synthesis. Llama 3 is available in two sizes: 8B for streamlined deployment and development
        on consumer-grade GPUs, and 70B for extensive AI applications. Each size variant offers both base and instruction-tuned
        versions. Additionally, a new iteration of Llama Guard, fine-tuned on Llama 3 8B, has been introduced as Llama Guard 2.
      </p>
      <h3>What Is Qdrant ?</h3>
      <p>
        Qdrant is a vector similarity search engine that offers a production-ready service through an easy-to-use API. It
        specializes in storing, searching, and managing points (vectors) along with additional payload information. It is
        optimized for efficiently storing and querying high-dimensional vectors. Vector databases like Qdrant leverage specialized
        data structures and indexing techniques such as Hierarchical Navigable Small World (HNSW) for implementing Approximate
        Nearest Neighbors and Product Quantization, among others. These optimizations enable fast similarity and semantic search,
        allowing users to locate vectors that closely match a given query vector based on a specified distance metric. Commonly
        used distance metrics supported by Qdrant includeEuclidean Distance, Cosine Similarity, and Dot Product.
      </p>

      <h2>Technology Stack Used</h2>
      <ul>
        <li>Llama-Index</li>
        <li>Llama-Index-Embeddings-Fastembed</li>
        <li>Llama-Index-LLMs-Huggingface</li>
        <li>Llama-Index-Vector-Stores-Qdrant</li>
        <li>Sentence-Transformers</li>
        <li>Qdrant</li>
        <li>FastembedUnstructured</li>
        <li>Einops</li>
      </ul>

      <h2>Code Implementation</h2>
      <h3>Install Required Libraries</h3>
      ```python
      %%writefile requirements.txt
      llama-index
      llama-index-llms-huggingface
      llama-index-embeddings-fastembed
      fastembedUnstructured[md]
      qdrant
      llama-index-vector-stores-qdrant
      inops
      accelerate
      sentence-transformers
      ```
      ```
      pip install -r requirements.txt
      accelerate==0.29.3
      einops==0.7.0
      sentence-transformers==2.7.0
      transformers==4.39.3
      qdrant-client==1.9.0
      llama-index==0.10.32
      llama-index-agent-openai==0.2.3
      llama-index-cli==0.1.12
      llama-index-core==0.10.32
      llama-index-embeddings-fastembed==0.1.4
      llama-index-legacy==0.9.48
      llama-index-llms-huggingface==0.1.4
      llama-index-vector-stores-qdrant==0.2.8
      ```
      <h3>Download the Dataset</h3>
      ```
      !mkdir Data
      !wget "https://arxiv.org/pdf/1810.04805.pdf" -O Data/arxiv.pdf
      ```
      <h3>Load the Documents</h3>
      ```python
      from llama_index.core import SimpleDirectoryReader
      documents = SimpleDirectoryReader("/content/Data").load_data()
      ```
      <h3>Instantiate the Embedding Model</h3>
      ```python
      from llama_index.embeddings.fastembed import FastEmbedEmbedding
      from llama_index.core import Settings
      #embed_model = FastEmbedEmbedding(model_name="BAAI/bge-small-en-v1.5")
      #Settings.embed_model = embed_model
      #Settings.chunk_size = 512
      #
      ```
      <h3>Define the System Prompt</h3>
      ```python
      from llama_index.core import PromptTemplates
      system_prompt = "You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided."
      # This will wrap the default prompts that are internal to llama-index
      query_wrapper_prompt = PromptTemplate("<|USER|>{query_str}<|ASSISTANT|>")
      ```
      <h3>Instantiate the LLM</h3>
      Since we are using Llama 3 as the LLM, we need to do the following:
      ```python
      from huggingface_hub import notebook_login
      notebook_login()
      import torch
      from transformers import AutoModelForCausalLM, AutoTokenizer
      from llama_index.llms.huggingface import HuggingFaceLLM
      tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")
      stopping\n<h1>Section 1: Understanding RAG</h1>

    <h2>What is RAG?</h2>

    **RAG** stands for Retrieval Augmented Generation. It combines **retrieval-based** and **generative** approaches in natural language processing (NLP). RAG systems leverage **retrieval** and **generation** components:


    <ul>
      <li><b>Retrieval:</b> Finds relevant documents from a knowledge base given a query.</li>
      <li><b>Generation:</b> Generates a response based on the retrieved documents.</li>
    </ul>
    ```

    <h2>How does RAG work?</h2>

    RAG follows a two-step process:

    **1. Retrieval:**

    <ul>
      <li>Uses a retriever to find relevant documents for a query based on vector similarity.</li>
    </ul>
    ```

    **2. Generation:**

    <ul>
      <li>Uses a generator to generate a response based on the retrieved documents.</li>
      <li>The generator is trained on a large text and code dataset (e.g., transformer-based language model).</li>
      <li>Integrates factual information from the retrieved documents into the generated response.</li>
    </ul>
    ```

    <h2>Benefits of RAG:</h2>

    RAG offers advantages over traditional approaches:


    <ul>
      <li><b>Improved Accuracy:</b> Leverages factual knowledge from retrieved documents, reducing hallucinations.</li>
      <li><b>Enhanced Fluency:</b> Learns from the structure and style of retrieved documents, improving response quality.</li>
      <li><b>Reduced Hallucination:</b> Relies on factual information, mitigating false information generation.</li>
    </ul>
    ```\n## Section 2: Building a RAG System

    ### Prerequisites

    Before you can build a RAG system, you will need to install the following software:

    - Python 3.6 or later
    - PyTorch 1.0 or later
    - Transformers 4.0 or later
    - Llama-Index 0.10 or later
    - Llama 3
    - Qdrant

    ### Step 1: Create a knowledge base

    The first step is to create a knowledge base that you will use to train your RAG system. The knowledge base can be any collection of documents, such as a set of web pages, news articles, or scientific papers.

    ### Step 2: Preprocess the knowledge base

    Once you have created a knowledge base, you will need to preprocess it so that it can be used by your RAG system. This involves tokenizing the documents, converting them to vectors, and indexing them.

    ### Step 3: Train the retriever

    The next step is to train the retriever. The retriever will be used to find relevant documents for a given query. You can train the retriever using any of the following methods:

    - Supervised learning: This method involves training the retriever on a dataset of queries and their corresponding relevant documents.
    - Unsupervised learning: This method involves training the retriever on a dataset of unlabeled documents.

    ### Step 4: Train the generator

    The final step is to train the generator. The generator will be used to generate responses based on the retrieved documents. You can train the generator using any of the following methods:

    - Supervised learning: This method involves training the generator on a dataset of queries and their corresponding human-generated responses.
    - Unsupervised learning: This method involves training the generator on a dataset of unlabeled text.\n<h2>Section 3: Using a RAG System</h2>

    <h3>How to use a RAG system</h3>

    <ol>
      <li>**Prepare your query:** The first step is to prepare your query. Your query can be any question or statement that you want the RAG system to answer or respond to.</li>
      <li>**Submit your query to the RAG system:** Once you have prepared your query, you can submit it to the RAG system. The RAG system will use the retriever to find relevant documents for your query and the generator to generate a response based on the retrieved documents.</li>
      <li>**Evaluate the response:** Once the RAG system has generated a response, you should evaluate the response to see if it is accurate and fluent. If the response is not satisfactory, you can try reformulating your query or using a different RAG system.</li>
    </ol>

    <h3>Applications of RAG systems</h3>

    RAG systems can be used for a variety of NLP tasks, including:

    * Question answering: RAG systems can be used to answer questions by finding relevant documents and generating a response based on those documents.
    * Summarization: RAG systems can be used to summarize text by finding the most important points in a document and generating a summary based on those points.
    * Machine translation: RAG systems can be used to translate text from one language to another by finding relevant documents in the target language and generating a translation based on those documents.\n```html
    <h1>Conclusion</h1>

    <p>RAG systems are a powerful tool for NLP tasks. They can be used to improve the accuracy, fluency, and factuality of LLM-generated responses. In this blog post, we have provided a step-by-step guide to building and using a RAG system. We hope that this guide will help you to build your own RAG system and use it to solve a variety of NLP problems.</p>

    <h2>Key Points</h2>

    <ol>
    <li>RAG systems are a combination of retrieval and generation models.</li>
    <li>RAG systems can improve the accuracy, fluency, and factuality of LLM-generated responses.</li>
    <li>RAG systems are easy to build and use.</li>
    <li>RAG systems can be used to solve a variety of NLP problems.</li>
    </ol>

  </body>
</html>